{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN, Activation Function, Batch Norm 과제\n",
    "> 인공지능 스터디 다섯 번째 과제에 오신 것을 환영합니다! 강의를 들으면서 배운 다양한 지식들을 실습을 통해서 활용해볼 시간을 가질 것입니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 필요한 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\박동욱\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.4.1+cu124)\n",
      "Requirement already satisfied: torchvision in c:\\users\\박동욱\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.19.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\박동욱\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\박동욱\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\박동욱\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\박동욱\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\박동욱\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\박동욱\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\박동욱\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\박동욱\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (2.1.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\박동욱\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\박동욱\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\박동욱\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch**는 PyTorch의 핵심 패키지로, 딥러닝 모델 구현에 필요한 기본 요소들을 포함하고있습니다.\n",
    "\n",
    "**torch.nn**은 신경망 모델 구현에 필요한 다양한 층(Layer)과 함수들을 포함\n",
    "\n",
    "**torchvision**은 Computer vision에서 사용하는 각종 테크닉들을 torch와 연동하여 구현한 라이브러리입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**numpy**는 행렬 연산을 위한 기본 수치 계산 라이브러리\n",
    "\n",
    "**datetime**는 시간 기록용 라이브러리\n",
    "\n",
    "**os**, **sys**는 운영체제 관련 정보 접근을 위한 라이브러리\n",
    "\n",
    "**matplotlib**는 데이터 시각화 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import imshow, imsave\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'CNN'\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU가 있다면 GPU를 통해 학습을 가속화하고, 없으면 CPU로 학습하기 위해 device를 정해준다.\n",
    "\n",
    "**torch.cuda.is_avaliable()** 는 GPU가 사용가능한지를 판단하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyCNN(nn.Module):\n",
    "    \"\"\"\n",
    "        Simple CNN Clssifier\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MyCNN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            # (N, 1, 28, 28)\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            # (N, 32, 14, 14)\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            # (N, 64, 7, 7)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(7*7*64, 512),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y_ = self.conv(x) # (N, 64, 7, 7)\n",
    "        y_ = y_.view(y_.size(0), -1) # (N, 64*7*7)\n",
    "        y_ = self.fc(y_)\n",
    "        return y_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 정의를 위한 코드\n",
    "\n",
    "**nn.Module**: 뉴럴넷 구현을 위한 base class. forward, parameter 등 모델을 만들고 사용할 때 필요한 부분들이 내부적으로 구현되어 있음.\n",
    "\n",
    "**__init__**: python class의 constructor. 필요한 멤버변수들을 초기화하고, **nn.Sequential** or **nn.ModuleList**를 이용하여 모델의 구성을 정의한다.\n",
    "\n",
    "**forward**: 모델의 input을 받고 output을 return하는 함수.\n",
    "\n",
    "**nn.Conv2d**: Convolutional Layer. 입력 채널과 출력 채널을 parameter로 받는다. kernel size가 3이므로 padding 1을 통해 같은 크기가 나오도록 한다.\n",
    "\n",
    "**nn.MaxPool2d**: max pooling을 수행한다. feature map size를 줄여주는 역할.\n",
    "\n",
    "**nn.Dropout**: Dropout. p는 drop probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **사전지식**\n",
    "\n",
    "nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=1)의 주요 인자들:\n",
    "\n",
    "in_channels: 입력 채널 수\n",
    "out_channels: 출력 채널 수\n",
    "kernel_size: 컨볼루션 필터의 크기\n",
    "stride: 필터가 이동하는 간격\n",
    "padding: 입력 데이터 주변을 특정 값으로 채우는 크기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓ <font color='red'><b>[ 퀴즈 ]</b></font> 커널 사이즈 이해하기\n",
    "다음 상황에서 출력 feature map의 크기를 계산해보세요.\n",
    "\n",
    "입력 이미지 크기: 28x28\n",
    "\n",
    "커널 사이즈: 5x5\n",
    "\n",
    "stride: 1\n",
    "\n",
    "padding: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ! <font color='green'><b>[ 정답 ]</b></font> 작성해주세요\n",
    "24 x 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓ <font color='red'><b>[ 퀴즈 ]</b></font> ReLU의 역할\n",
    "ReLU의 역할을 설명해주세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ! <font color='green'><b>[ 정답 ]</b></font> 작성해주세요\n",
    "활성화 함수로서 신경망을 쌓는 역할"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓ <font color='red'><b>[ 퀴즈 ]</b></font> MaxPooling 계산\n",
    "\n",
    "MaxPool2d(2, 2)를 적용할 때, 다음 feature map의 크기 변화를 계산해보세요:\n",
    "입력 feature map: 14x14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ! <font color='green'><b>[ 정답 ]</b></font> 작성해주세요\n",
    "7 x 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = MyCNN().to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정의한 모델을 메모리에 올리는 작업."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리\n",
    "### 데이터 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.1307,), (0.3081,)),]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓ <font color='red'><b>[ 생각해보기 ]</b></font> 정규화\n",
    "\n",
    "입력 정규화가 왜 필요할까요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hint : transforms.Normalize((0.1307,), (0.3081,))는 MNIST 데이터셋의 평균(mean)과 표준편차(standard deviation)를 사용한 정규화입니다.\n",
    "\n",
    "입력 데이터의 사이즈 조절, 너무 큰 값이 있을 때, 그 값을 조정할 수 있고 전체적인 데이터의 스케일을 조정하여 학습 시간을 단축하기 위함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 로드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1000)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../data/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:02<00:00, 4757440.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST\\raw\\train-images-idx3-ubyte.gz to ../data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1000)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 129536.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST\\raw\\train-labels-idx1-ubyte.gz to ../data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1000)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:01<00:00, 1583657.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST\\raw\\t10k-images-idx3-ubyte.gz to ../data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1000)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ../data/MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mnist_train = datasets.MNIST(root='../data/', train=True, transform=transform, download=True)\n",
    "mnist_test = datasets.MNIST(root='../data/', train=False, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "print(mnist_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_train.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**datasets**에는 여러 데이터들에 대해 다운로드하고 처리하는 클래스가 내장되어 있음. [참고](https://pytorch.org/docs/stable/torchvision/datasets.html)\n",
    "\n",
    "root 폴더에 없을 시에 download하고, 앞서 정의한 transform에 따라 전처리 된 데이터를 return함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 로더 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(dataset=mnist_test, batch_size=100, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DataLoader**는 pytorch에서 학습 시에 데이터를 배치 사이즈만큼씩 효율적으로 불러오도록 돕는 클래스. 잘 사용할수록 GPU의 사용률이 올라간다.\n",
    "\n",
    "**shuffle**: every epochs 마다 데이터의 순서를 랜덤하게 섞는다.\n",
    "\n",
    "**drop_last**: 데이터의 개수가 배치 사이즈로 나눠떨어지지 않는 경우, 마지막 배치를 버린다. 주로 학습시에만 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() # 손실 함수 (내부적으로 softmax 포함)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.001)  # Adam 옵티마이저"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**nn.CrossEntropyLoss**: Cross entropy를 계산하는 Loss. softmax가 내부적으로 수행된다.\n",
    "\n",
    "**optim.Adam**: optim에는 여러 optimizer가 있고, Adam Optimizer는 대표적으로 많이 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최대 epoch 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_epoch = 5\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "첫번째 for문 - 원하는 epoch만큼 반복\n",
    "\n",
    "두번째 for문 - training datset에서 배치 사이즈 만큼씩 모두 샘플링 될 때까지 반복.\n",
    "\n",
    "**Line 2**: MNIST dataset은 DataLoader를 통해 image와 label을 return.\n",
    "\n",
    "**Line 4**: 각각 Device에 올린다 (GPU or CPU)\n",
    "\n",
    "**Line 5**: 모델에 이미지를 넣고 forward propagation 한다.\n",
    "\n",
    "**Line 7**: 결과값 y_hat과 실제 정답 y에 대한 loss를 계산한다.\n",
    "\n",
    "**zero_grad (Line 9)**: 모델의 gradient를 0으로 초기화한다.\n",
    "\n",
    "**backward (Line 10)**: loss를 계산하는 것까지 연결되어있는 graph를 따라 gradient를 계산한다.\n",
    "\n",
    "**step (Line 11)**: 계산된 gradient를 모두 parameter에 적용한다.\n",
    "\n",
    "**eval (Line 17)**: 모델을 evaluation mode로 바꿔준다 (dropout 조정, Batch normalization 조정 등)\n",
    "\n",
    "**torch.no_grad (Line 19)**: 그래디언트 계산 비활성화\n",
    "\n",
    "**torch.max (Line 24)**: max value와 indices(즉, argmax)를 return.\n",
    "\n",
    "**train (Line 29)**: evaluation mode였던 모델을 train mode로 전환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/5, Step: 0, Loss: 2.339866876602173\n",
      "******************** Test ********************\n",
      "Step: 0, Loss: 3.0538909435272217, Accuracy: 9.75 %\n",
      "**********************************************\n",
      "Epoch: 0/5, Step: 500, Loss: 0.05693332478404045\n",
      "Epoch: 1/5, Step: 1000, Loss: 0.013683677650988102\n",
      "******************** Test ********************\n",
      "Step: 1000, Loss: 0.008388352580368519, Accuracy: 98.48 %\n",
      "**********************************************\n",
      "Epoch: 1/5, Step: 1500, Loss: 0.01550370641052723\n",
      "Epoch: 2/5, Step: 2000, Loss: 0.0069448454305529594\n",
      "******************** Test ********************\n",
      "Step: 2000, Loss: 0.0026775775477290154, Accuracy: 98.54 %\n",
      "**********************************************\n",
      "Epoch: 2/5, Step: 2500, Loss: 0.04648410156369209\n",
      "Epoch: 3/5, Step: 3000, Loss: 0.05799373239278793\n",
      "******************** Test ********************\n",
      "Step: 3000, Loss: 0.0015220472123473883, Accuracy: 98.89 %\n",
      "**********************************************\n",
      "Epoch: 3/5, Step: 3500, Loss: 0.0214501041918993\n",
      "Epoch: 4/5, Step: 4000, Loss: 0.0018870227504521608\n",
      "******************** Test ********************\n",
      "Step: 4000, Loss: 0.0016441534971818328, Accuracy: 98.76 %\n",
      "**********************************************\n",
      "Epoch: 4/5, Step: 4500, Loss: 0.014279419556260109\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epoch):\n",
    "    for idx, (images, labels) in enumerate(train_loader):\n",
    "        # Training Discriminator\n",
    "        x, y = images.to(DEVICE), labels.to(DEVICE) # (N, 1, 28, 28), (N, )\n",
    "        y_hat = model(x) # (N, 10)\n",
    "        \n",
    "        loss = criterion(y_hat, y)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        if step % 500 == 0:\n",
    "            print('Epoch: {}/{}, Step: {}, Loss: {}'.format(epoch, max_epoch, step, loss.item()))\n",
    "        \n",
    "        if step % 1000 == 0:\n",
    "            model.eval()\n",
    "            acc = 0.\n",
    "            with torch.no_grad():\n",
    "                for idx, (images, labels) in enumerate(test_loader):\n",
    "                    x, y = images.to(DEVICE), labels.to(DEVICE) # (N, 1, 28, 28), (N, )\n",
    "                    y_hat = model(x) # (N, 10)\n",
    "                    loss = criterion(y_hat, y)\n",
    "                    _, indices = torch.max(y_hat, dim=-1)\n",
    "                    acc += torch.sum(indices == y).item()\n",
    "            print('*'*20, 'Test', '*'*20)\n",
    "            print('Step: {}, Loss: {}, Accuracy: {} %'.format(step, loss.item(), acc/len(mnist_test)*100))\n",
    "            print('*'*46)\n",
    "            model.train()\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓ <font color='red'><b>[ 퀴즈 ]</b></font> zero_grad()와 no_grad()의 차이\n",
    "\n",
    "optim.zero_grad()와 torch.no_grad()의 차이를 생각해보면서 이 코드에서는 어떤 주기로 평가되는지 서술하시오"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ! <font color='green'><b>[ 정답 ]</b></font> 작성해주세요\n",
    "\n",
    "학습 데이터 500개 주기로 출력을 한다. 학습데이터 1000개 단위로 테스트 데이터에서 테스트를 한다.   \n",
    "zero_grad() 함수가 있는 부분은 모델이 학습을 하는 부분이고, no_grad() 부분은 모델을 테스트하기 위한 부분이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and Visualize\n",
    "\n",
    "학습시킨 모델이 얼만큼의 성능을 가지는지 테스트해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** Test ********************\n",
      "Step: 4685, Loss: 0.004865661729127169, Accuracy: 98.92 %\n",
      "**********************************************\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "model.eval()\n",
    "acc = 0.\n",
    "with torch.no_grad():\n",
    "    for idx, (images, labels) in enumerate(test_loader):\n",
    "        x, y = images.to(DEVICE), labels.to(DEVICE) # (N, 1, 28, 28), (N, )\n",
    "        y_hat = model(x) # (N, 10)\n",
    "        loss = criterion(y_hat, y)\n",
    "        _, indices = torch.max(y_hat, dim=-1)\n",
    "        acc += torch.sum(indices == y).item()\n",
    "print('*'*20, 'Test', '*'*20)\n",
    "print('Step: {}, Loss: {}, Accuracy: {} %'.format(step, loss.item(), acc/len(mnist_test)*100))\n",
    "print('*'*46)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7777번째 사진의 차원과, 정답값을 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 28, 28]), 5)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 7777 # 0 to 9999\n",
    "img, y = mnist_test[idx]\n",
    "img.shape, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어떻게 생겼는지도 궁금하군요. 확인해볼까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ca752fa3c0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow(img[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = img.unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**unsqueeze**: 원하는 dim에 차원을 높여준다. 단일 이미지를 배치 형태로 변환 (N, C, H, W)해주는 과정으로 CNN forward를 위해서는 4D-Tensor여야합니다. 우리가 이제껏 봤던 RGB 이미지 데이터셋은 3차원이지만, 데이터의 개수를 고려해주는 N으로 한번 더 넓혔다고 생각해볼 수 있겠고요, 현재는 mnist gray scale dataset이므로 흑백차원이므로 C가 Grayscale: 1채널이라고 생각하면 될 것 같습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 학습시켰던 모델에 sample을 넣어봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = img.to(DEVICE)\n",
    "out = model(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "out값을 확인해볼까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -6.2772,  -2.3315,  -9.1759,  10.4449,  -8.8060,  22.2153,   1.4116,\n",
      "         -10.0633,   0.0345,  -0.9792]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가장 큰 값이 무엇일까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, idx = out.max(dim=-1)\n",
    "idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 학습시켰던 가중치를 pkl로 저장하면서 마무리해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving params.\n",
    "torch.save(model.state_dict(), 'model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "음. 그런데 끝내기 전에 우리가 5주차에 배웠던 내용을 생각해보면 뭔가 언급이 안되었던 부분이 있었던 것 같은데요?\n",
    "정의된 MyCNN을 볼까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNN(nn.Module):\n",
    "    \"\"\"\n",
    "        Simple CNN Clssifier\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MyCNN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            # (N, 1, 28, 28)\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            # (N, 32, 14, 14)\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            # (N, 64, 7, 7)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(7*7*64, 512),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y_ = self.conv(x) # (N, 64, 7, 7)\n",
    "        y_ = y_.view(y_.size(0), -1) # (N, 64*7*7)\n",
    "        y_ = self.fc(y_)\n",
    "        return y_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "뭐가 빠졌죠?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "음..\n",
    "\n",
    "\n",
    ".\n",
    "\n",
    "\n",
    ".\n",
    "\n",
    "\n",
    ".\n",
    "\n",
    "\n",
    "🤔\n",
    "\n",
    ".\n",
    "\n",
    "\n",
    ".\n",
    "\n",
    "\n",
    ".\n",
    "\n",
    "\n",
    ".\n",
    "\n",
    "\n",
    "🤔\n",
    "\n",
    ".\n",
    "\n",
    "\n",
    ".\n",
    "\n",
    "\n",
    ".\n",
    "\n",
    "\n",
    ".\n",
    "\n",
    "\n",
    "🤔\n",
    "\n",
    ".\n",
    "\n",
    "\n",
    ".\n",
    "\n",
    "\n",
    ".\n",
    "\n",
    "\n",
    ".\n",
    "\n",
    "\n",
    "🤔\n",
    "\n",
    "아!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 맞습니다. BatchNorm이 없네요?\n",
    "이 CNN 모델에 BatchNorm이 포함되지 않은 것은 의도적인 선택으로 보이는데요, BatchNorm을 써야하는 상황은\n",
    "\n",
    "### 1. 깊은 신경망 구조\n",
    "\n",
    "레이어가 많은 심층 네트워크\n",
    "Internal Covariate Shift 문제 해결 필요\n",
    "\n",
    "\n",
    "### 2. 불안정한 학습 과정\n",
    "\n",
    "학습이 불안정하거나 수렴이 느린 경우\n",
    "그래디언트 소실/폭발 문제 발생 시\n",
    "\n",
    "\n",
    "### 3. 큰 배치 사이즈\n",
    "\n",
    "배치 사이즈가 충분히 클 때 효과적\n",
    "일반적으로 32 이상 권장\n",
    "\n",
    "\n",
    "### 4. 높은 학습률 필요\n",
    "\n",
    "빠른 학습을 위해 높은 learning rate 사용 시\n",
    "학습 안정성 확보 필요\n",
    "\n",
    "으로 정리할 수 있을 것 같습니다. \n",
    "\n",
    "이 MNIST 예제의 경우 간단한 구조와 데이터셋 특성상 BatchNorm 없이도 충분히 학습 가능합니다.\n",
    "\n",
    "하지만 코드를 안보고 넘어가면 아쉬우니까 다음 코드에다 BatchNorm을 추가해서 수정해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNN(nn.Module):\n",
    "    \"\"\"\n",
    "        Simple CNN Clssifier\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MyCNN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            # (N, 1, 28, 28)\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            # (N, 32, 14, 14)\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            # (N, 64, 7, 7)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(7*7*64, 512),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y_ = self.conv(x) # (N, 64, 7, 7)\n",
    "        y_ = y_.view(y_.size(0), -1) # (N, 64*7*7)\n",
    "        y_ = self.fc(y_)\n",
    "        return y_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번주에 Low-Rank Adaptation (LoRA)도 학습해봤는데요, 관심있으신 분들은 난이도에 맞게 아래 자료도 읽어보시면 좋을 것 같아요!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 음.. 난 아직 논문까지는 어려워.. 😕😕\n",
    "**Easy Mode** : 논문 리뷰 읽기 [https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/lora/]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한번 읽어볼까? 얼마나 어렵겠어? 😎😎\n",
    "\n",
    "Hard Mode : 논문 원본 읽기 [https://arxiv.org/abs/2106.09685]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎉🎉🎉 5주차 과제 완료! 🎉🎉🎉\n",
    "```python\n",
    "🐙\n",
    "여러분 모두 수고 했어요!! 이번주 과제를 complete한 당신에게 행운을 드립니다. \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
